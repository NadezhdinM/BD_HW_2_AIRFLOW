# Отработка навыков работы с Airflow

Этот репозиторий содержит DAG для Apache Airflow, предназначенный для обработки данных о транзакциях клиентов и генерации флагов активности для различных продуктов. Процесс включает извлечение, трансформацию и загрузку данных (ETL) и демонстрирует параллельное выполнение задач для повышения эффективности.

## Возможности
- **Ежемесячный запуск по расписанию**: DAG настроен на запуск 5-го числа каждого месяца в полночь.
- **ETL-процесс**: Извлекает данные из CSV-файла, трансформирует их с использованием внешнего Python-скрипта и загружает результаты в объединённый CSV-файл.
- **Параллельная обработка**: Задачи настроены на параллельное выполнение для каждого продукта (от a до j).
- **Обработка ошибок**: Включает логирование и передачу данных через XCom для отладки.

## Структура проекта
```plaintext
/opt/airflow/your_project/
├── dags/
│    └── dag.py               # Основной файл определения DAG
├── scripts/
│    └── transform_script.py  # Логика трансформации, предоставленная дата-саентистами
├── data/
│    └── profit_table.csv     # Входной файл с данными о транзакциях
│    └── flags_activity.csv   # Выходной файл с объединёнными флагами активности
```

## Предварительные требования

1. **Apache Airflow**
   - Установите Airflow с помощью pip:
     ```bash
     pip install apache-airflow
     ```
   - Инициализируйте базу данных Airflow:
     ```bash
     airflow db init
     ```

2. **Настройка директорий**
   Убедитесь, что существует следующая структура в домашней директории Airflow (`AIRFLOW_HOME`):
   - Поместите `dag.py` в папку `dags/`.
   - Поместите `transform_script.py` в папку `scripts/`.
   - Поместите входной CSV-файл (`profit_table.csv`) в папку `data/`.

3. **Python-библиотеки**
   Убедитесь, что в вашем окружении установлены необходимые библиотеки:
   - `pandas`
   - `subprocess`
   - `logging`

## Конфигурация

### Параметры DAG
- **ID DAG**: `product_processing_dag`
- **Расписание**: Запуск 5-го числа каждого месяца (`0 0 5 * *`).
- **Повторы**: 1 повтор с задержкой в 5 минут.
- **Дата начала**: 1 ноября 2024 года.

### Скрипт трансформации
Логика трансформации находится в файле `transform_script.py`. Он обрабатывает фильтрованные данные для каждого продукта и выводит результат для объединения.

## Как запустить

1. **Запустите веб-сервер Airflow**
   ```bash
   airflow webserver --port 8080
   ```

2. **Запустите планировщик**
   ```bash
   airflow scheduler
   ```

3. **Откройте веб-интерфейс**
   Перейдите по адресу [http://localhost:8080](http://localhost:8080) в вашем браузере.

4. **Активируйте DAG**
   - Найдите `product_processing_dag` в списке.
   - Включите DAG, щелкнув на переключатель рядом с его названием.

5. **Запустите DAG вручную** (по желанию)
   - Перейдите на страницу деталей DAG.
   - Нажмите кнопку "Trigger DAG" (Запустить DAG).

## Результат
Итоговые данные будут добавлены в файл `flags_activity.csv` в папке `data/`. Убедитесь, что директория имеет права на запись для процесса Airflow.

## Логи и отладка
- Логи для отдельных задач можно просмотреть через веб-интерфейс Airflow.
- Проверьте значения `xcom_push` и `xcom_pull` для отладки данных, передаваемых между задачами.

## Примечания
- Убедитесь, что входной файл данных (`profit_table.csv`) обновляется ежемесячно.
- Если вы используете Docker, смонтируйте необходимые директории как тома, чтобы обеспечить доступность файлов.

## Возможные улучшения
- Реализовать автоматическую проверку данных входного CSV-файла.
- Добавить мониторинг с уведомлениями для неудачных задач.
- Оптимизировать логику трансформации для больших наборов данных.

## Лицензия
Этот проект лицензирован на условиях лицензии MIT.
